<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Wenyan Li" />

  
  
  
    
  
  <meta name="description" content="" />

  
  <link rel="alternate" hreflang="en-us" href="https://wenyanli.org/publication/" />

  
  
  
    <meta name="theme-color" content="#bbdefb" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.2215a8106e8600c1296dee0a44c6a5ab.css" />

  



  


  


  




  
  
  

  
    <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="Wenyan Li" />
  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://wenyanli.org/publication/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Wenyan Li" />
  <meta property="og:url" content="https://wenyanli.org/publication/" />
  <meta property="og:title" content="Publications | Wenyan Li" />
  <meta property="og:description" content="" /><meta property="og:image" content="https://wenyanli.org/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://wenyanli.org/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2024-06-16T00:00:00&#43;00:00" />
    
  

  



  

  





  <title>Publications | Wenyan Li</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  dark " data-wc-page-id="3a079e7dad19be978a318345a7749d34" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.7139d4fb8f74c01b97e65e19c846e5cc.js"></script>

  




  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Wenyan Li</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Wenyan Li</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/uploads/wenyanli_cv.pdf"><span>CV</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    















  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search form-control form-control-sm" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            
            <option value=".pubtype-1">
              Conference paper
            </option>
            
            <option value=".pubtype-8">
              Patent
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2024">
              2024
            </option>
            
            <option value=".year-2023">
              2023
            </option>
            
            <option value=".year-2022">
              2022
            </option>
            
            <option value=".year-2020">
              2020
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Wenyan Li</span>, <span >
      Xinyu Zhang</span>, <span >
      Jiaang Li</span>, <span >
      Qiwei Peng</span>, <span >
      Raphael Tang</span>, <span >
      Li Zhou</span>, <span >
      Weijia Zhang</span>, <span >
      Guimin Hu</span>, <span >
      Yifei Yuan</span>, <span >
      Anders Søgaard</span>, <span >
      Daniel Hershcovich</span>, <span >
      Desmond Elliott</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>EMNLP 2024</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/emnlp2024/" >
    <div class="img-hover-zoom">
      <img src="/publication/emnlp2024/featured_hu85ed6b1168dfc1bd0bd48c7b3785522f_199606_808x455_fill_q75_h2_lanczos_smart1_3.webp" height="455" width="808"
           class="article-banner" alt="FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture" loading="lazy">
    </div>
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/emnlp2024/" >FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture</a>
  </div>

  
  <a href="/publication/emnlp2024/"  class="summary-link">
    <div class="article-style">
      <p>Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision-language Models (VLMs) and large language models (LLMs) on newly collected, unseen food images and corresponding questions. FoodieQA comprises three multiple-choice question-answering tasks where models need to answer questions based on multiple images, a single image, and text-only descriptions, respectively.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2406.11030" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/emnlp2024/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/lyan62/FoodieQA" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/lyan62/FoodieQA" target="_blank" rel="noopener">
  Dataset
</a>













  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Wenyan Li</span>, <span >
      Jiaang Li</span>, <span >
      Rita Ramos</span>, <span >
      Raphael Tang</span>, <span >
      Desmond Elliott</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>ACL 2024</em> (oral, 8%)
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/acl2024/" >
    <div class="img-hover-zoom">
      <img src="/publication/acl2024/featured_hu68efc53be7e7ab296734937f7d29d582_262471_808x455_fill_q75_h2_lanczos_smart1_3.webp" height="455" width="808"
           class="article-banner" alt="Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning" loading="lazy">
    </div>
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/acl2024/" >Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning</a>
  </div>

  
  <a href="/publication/acl2024/"  class="summary-link">
    <div class="article-style">
      <p>Recent advances in retrieval-augmented models for image captioning highlight the benefit of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice: the retrieved information can sometimes mislead the model, resulting in incorrect generation and worse performance. In this paper, we analyze the robustness of a retrieval-augmented captioning model SmallCap. Our analysis shows that the model is sensitive to tokens that appear in the majority of the retrieved captions, and the input attribution shows that those tokens are likely copied into the generated output. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This decreases the chance that the model learns to copy majority tokens, and improves both in-domain and cross-domain performance.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/acl2024/2406.02265v2.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/acl2024/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/lyan62/RobustCap" target="_blank" rel="noopener">
  Code
</a>














  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Wenyan Li</span>, <span >
      Jonas F Lotz</span>, <span >
      Chen Qiu</span>, <span >
      Desmond Elliott</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>EACL 2024</em> (oral)
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/sd-synth2023/" >The Role of Data Curation in Image Captioning</a>
  </div>

  
  <a href="/publication/sd-synth2023/"  class="summary-link">
    <div class="article-style">
      <p>Image captioning models are typically trained by treating all samples equally, neglecting to account for mismatched or otherwise difficult data points. In contrast, recent work has shown the effectiveness of training models by scheduling the data using curriculum learning strategies. This paper contributes to this direction by actively curating difficult samples in datasets <em>without</em> increasing the total number of samples. We explore the effect of using three data curation methods within the training process: complete removal of an sample, caption replacement, or image replacement via a text-to-image generation model. Experiments on the Flickr30K and COCO datasets with the BLIP and BEiT-3 models demonstrate that these curation methods do indeed yield improved image captioning models, underscoring their efficacy.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/sd-synth2023/wli_EACL_2024_preprint.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sd-synth2023/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/lyan62/data-curation/" target="_blank" rel="noopener">
  Code
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1yBO_tA_F5vBeVC3lETbcX7rN6nQIIC2M/view?usp=sharing" target="_blank" rel="noopener">
  Slides
</a>







  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Wenyan Li</span>, <span >
      Dong Li</span>, <span >
      Wanjing Li</span>, <span >
      Yuanjie Wang</span>, <span >
      Hai Jie</span>, <span >
      Yiran Zhong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Learning with Small Data (LSD), 2023</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/lsd2023/" >MAP: Low-data Regime Multimodal Learning with Adapter-based Pre-training and Prompting</a>
  </div>

  
  <a href="/publication/lsd2023/"  class="summary-link">
    <div class="article-style">
      <p>Pretrained vision-language (VL) models have shown impressive results on various multi-modal downstream tasks recently. Many of the benchmark models build on pretrained causal language models (LMs), leveraging the original few-shot learning and generalization capability of the LMs trained with large text corpora. However, these models are often gigantic and require large-scale image and text data with high computational cost to train. This paper introduces a moderate-size model called MAP for efficient VL transfer learning through adapter-based pretraining and prompting. We aim to answer the question of how much we can complete through VL pretraining within the low-data regime while maximizing efficiency in transferring knowledge of a moderate-size frozen LM. Our experiments demonstrate that MAP achieves substantially better zero-shot and few-shot performance on downstream VL tasks with only 10% the size of pretraining data and a 30x lighter pretrained LM backbone compared to Frozen. MAP also outperforms fully trained models of comparable size at retaining its transfer learning ability when the amount of training data reduces.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/2023.clasp-1.19.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/lsd2023/cite.bib">
  Cite
</a>















  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-8 year-2022">
          












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Wenyan Li</span>, <span >
      Ferhan Ture</span>, <span >
      Jose Casillas</span>, <span >
      Tom Des Jardins</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>U.S. filed patent</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/patent_wli_2021/" >Systems and Methods for Training Voice Query Models</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/patent_wli_2021/cite.bib">
  Cite
</a>















  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Wenyan Li</span>, <span >
      Ferhan Ture</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>SIGIR 2020</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/sigir2020/" >Auto-annotation for Voice-enabled Entertainment Systems</a>
  </div>

  
  <a href="/publication/sigir2020/"  class="summary-link">
    <div class="article-style">
      <p>Voice-activated intelligent entertainment systems are prevalent in modern TVs. These systems require accurate automatic speech recognition (ASR) models to transcribe voice queries for further downstream language understanding tasks. Currently, labeling audio data for training is the main bottleneck in deploying accurate machine learning ASR models, especially when these models require up-to-date training data to adapt to the shifting customer needs. We present an auto-annotation system, which provides high quality training data without any hand-labeled audios by detecting speech recognition errors and providing possible fixes. Through our algorithm, the auto-annotated training data reaches an overall word error rate (WER) of 0.002; furthermore, we obtained a reduction of 0.907 in WER after applying the auto-suggested fixes.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/sigir2020/sigir2020_annotation.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sigir2020/cite.bib">
  Cite
</a>









  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1A3UrASelpItdO8AbEYc0ShNJGpVm0R81/view?usp=sharing" target="_blank" rel="noopener">
  Slides
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1WX-pLczRgN5BDQgGGiChCU1xG2wiFj7b/view?usp=sharing" target="_blank" rel="noopener">
  Video
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3397271.3401241" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Wenyan Li</span>, <span >
      Alvin Grissom II</span>, <span >
      Jordan Boyd-Graber</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>EMNLP findings 2020</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/emnlp2020/" >An Attentive Recurrent Model for Incremental Prediction of Sentence-final Verbs</a>
  </div>

  
  <a href="/publication/emnlp2020/"  class="summary-link">
    <div class="article-style">
      <p>Verb prediction is important for understanding human processing of verb-final languages, with practical applications to real-time simultaneous interpretation from verb-final to verb-medial languages. While previous approaches use classical statistical models, we introduce an attention-based neural model to incrementally predict final verbs on incomplete sentences in Japanese and German SOV sentences. To offer flexibility to the model, we further incorporate synonym awareness. Our approach both better predicts the final verbs in Japanese and German and provides more interpretable explanations of why those verbs are selected.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/emnlp2020/2020.findings-emnlp.12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/emnlp2020/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.18653/v1/2020.findings-emnlp.12" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

        </div>

        
      </div>

    </div>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.92d2024afaa4dce0cad42ba360879ce9.js"></script>

    
    
    
      
        <script src="https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js" integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
      

      
      

      

      

    

    
    
    

    
    

    
    
    

    
    

    
    
    
    

    
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.e8fd2d733eef6a8bbbe0539398fc0547.js" type="module"></script>
    
    
    
    
    
    
    
    
    <script src="/en/js/wowchemy.min.b3b95b2ec614292e1913846b81375a58.js"></script>

    
    
    
    
    
    
      
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>






</body>
</html>
