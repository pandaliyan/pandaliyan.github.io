<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wenyan Li</title>
    <link>https://wenyanli.org/</link>
      <atom:link href="https://wenyanli.org/index.xml" rel="self" type="application/rss+xml" />
    <description>Wenyan Li</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 Jul 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wenyanli.org/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Wenyan Li</title>
      <link>https://wenyanli.org/</link>
    </image>
    
    <item>
      <title>Voice Query Auto Completion</title>
      <link>https://wenyanli.org/publication/emnlp2021/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/emnlp2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Attentive Recurrent Model for Incremental Prediction of Sentence-final Verbs</title>
      <link>https://wenyanli.org/publication/emnlp2020/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/emnlp2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Auto-annotation for Voice-enabled Entertainment Systems</title>
      <link>https://wenyanli.org/publication/sigir2020/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/sigir2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Systems and Methods for Training Voice Query Models</title>
      <link>https://wenyanli.org/publication/patent_wli_2021/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/patent_wli_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://wenyanli.org/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RNN review paper notes</title>
      <link>https://wenyanli.org/post/getting-started/</link>
      <pubDate>Wed, 13 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/post/getting-started/</guid>
      <description>&lt;h1 id=&#34;reading-notes-of-a-critical-review-of-recurrent-neural-networks-for-sequence-learninghttpsarxivorgpdf150600019pdf&#34;&gt;Reading notes of &lt;a href=&#34;https://arxiv.org/pdf/1506.00019.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Critical Review of Recurrent Neural Networks for Sequence Learning&lt;/a&gt;.&lt;/h1&gt;
&lt;h2 id=&#34;main-idea&#34;&gt;Main Idea&lt;/h2&gt;
&lt;p&gt;Answered the question of &lt;strong&gt;Why recurrent neural networks?&lt;/strong&gt; in aspects of Architectures, algorithms, results, comparison to alternative methodologies on sequence learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNNs are able to manage text sequences of arbitrary lengths. While processing each token/element in the sequence one at a time, they pass information across time steps.
&lt;ul&gt;
&lt;li&gt;SVM, logistic regression and feedforward networks proved to be useful under independece assumption which precludes modeling long-range dependencies.&lt;/li&gt;
&lt;li&gt;HMM has memeoryless property and has exponentially growing state space even when considering large context window which is computationally impractical for modelling long-range dependencies.&lt;/li&gt;
&lt;li&gt;RNNs are differentiable end to end given fixed architectures and regularization techniques can be used for preventing overfitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;neural-nets-and-rnns&#34;&gt;Neural Nets and RNNs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Neural nets: Neurons in the neural networks has values equal to the output of the activation function applied on the weighted sum of the input nodes&#39; values: $ v_{j} = l_{j}(\sum_{j&#39;}w_{jj&#39;}\cdot{v_{j&#39;}})$, which was then written as  $ v_{j} = \sigma(a_j)$
where $a_j = \sum_{j&#39;}w_{jj&#39;}\cdot{v_{j&#39;}}$ and $\sigma$ for sigmoid. And activation functions are task-specific, for example, softmax are normally used in multiclass problems.&lt;/li&gt;
&lt;li&gt;Feedforward and backprop
&lt;ul&gt;
&lt;li&gt;Goal of feedforward NNs: minimize loss function: $L(\hat{y}, y)$.&lt;/li&gt;
&lt;li&gt;Backprop for training: calculate derivatives of the loss function $L$ wrt the parameters of the network. Parameters are then updated by stochastic gradient descent (SGD), i.e. take steps of modifications towards the expecting direction which decreases the loss $L$. Note, loss function may be non-convex, thus global optimal is not gauranteed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RNNs: takes both current input $\mathbf{x}^{(t)}$ and hidden state from previous $\mathbf{h}^{(t-1)}$:
&lt;ul&gt;
&lt;li&gt;$h^{(t)} = \sigma(W^{hx}\mathbf{x}^{(t)}+W^{hh}\mathbf{h}^{(t-1)} + \mathbf{b}_h)$&lt;/li&gt;
&lt;li&gt;$\hat{\mathbf{y}}^{t}= softmax(W^{yh}\mathbf{h}^{(t)} + \mathbf{b}_y)$
With above forward network, backprop of RNN is called Backprop through time (BPTT).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Exploding/vanishing gradients: When learning long-distance dependencies, the gradients of weights may explode or vanish through the time steps. The following tutorial also gives a  detailed description. &lt;a href=&#34;https://arxiv.org/abs/1703.01619&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Machine Translation and Sequence-to-sequence Models: A Tutorial&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lstms-grus-bidirectional&#34;&gt;LSTMs, GRUs, Bidirectional&lt;/h2&gt;
&lt;p&gt;A memory cell and gates are introduced in LSTM, which has unit gradient when adding previous memory $\mathbf{c}_{t-1}$ to the current $\mathbf{c}_t$.
Bidirectional structure has one more layer of hidden nodes which takes inputs from the end, which allows context information before and after the current step.&lt;/p&gt;
&lt;p&gt;Gated recurrent units (GRUs) appear as a simpler and computationally easier alternative of LSTM, which was not introducted in this paper but has on par performance as LSTM.&lt;/p&gt;
&lt;h2 id=&#34;neural-turing-machines&#34;&gt;Neural Turing Machines&lt;/h2&gt;
&lt;p&gt;Incorporate external memory buffer compared to RNNs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Memory matrix +  controller(may be a rnn)&lt;/li&gt;
&lt;li&gt;differentiable end-to-end&lt;/li&gt;
&lt;li&gt;can backprop via SGD&lt;/li&gt;
&lt;li&gt;outperforms LSTM (without external memory) when generalizing on longer inputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;representing natural language inputs and outputs
&lt;ul&gt;
&lt;li&gt;character-level, word level, one-hot, word embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BLEU score for translation evaluation
&lt;ul&gt;
&lt;li&gt;vrevity penalty, ngram precisions&lt;/li&gt;
&lt;li&gt;METEOR as an alternative which is based on word to word matches&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;translation as a SEQ2SEQ problem&lt;/li&gt;
&lt;li&gt;image captioning:  input images and target captions as inputs. Encoding can be dong using CNN for images. Attention mechanism can be used in decoding.&lt;/li&gt;
&lt;li&gt;More: handwriting recognition, unsupervised video encoding, video captioning, computer programs reading.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;discussion&#34;&gt;discussion&lt;/h3&gt;
&lt;p&gt;I don&amp;rsquo;t include most of the equations in the notes. The paper is clear in why and how RNN makes a difference in learning long-dependencies, which serves pretty well as a review/tutorial from a higher level with math well explained.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Phenotype from Genomic Sequence with Deep Neural Networks</title>
      <link>https://wenyanli.org/project/example/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/project/example/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In Yu M. et al. (2016), phenotype is translated from genotype based on gene ontology, and predicted interaction scores
may be influenced by errors in gene annotations or relationship between terms. As deep learning being effective in
identifying complex patterns from feature-rich datasets, especially as recurrent neural networks(RNNs) such as long
short term memory(LSTM) and gated recurrent unit(GRU) are capable of dealing with long-distance sequential data,
predicting genetic interactions directly from DNA or amino-acid sequences using deep learning techniques would help us
gain insights into underlying complex phenotypes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://wenyanli.org/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
