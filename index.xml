<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wenyan Li</title>
    <link>https://wenyanli.org/</link>
      <atom:link href="https://wenyanli.org/index.xml" rel="self" type="application/rss+xml" />
    <description>Wenyan Li</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 16 Sep 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wenyanli.org/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Wenyan Li</title>
      <link>https://wenyanli.org/</link>
    </image>
    
    <item>
      <title>Lost in Embeddings: Information Loss in Vision-Language Models</title>
      <link>https://wenyanli.org/publication/emnlp2025-vlmloss/</link>
      <pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/emnlp2025-vlmloss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding</title>
      <link>https://wenyanli.org/publication/emnlp2025-revenea/</link>
      <pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/emnlp2025-revenea/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation</title>
      <link>https://wenyanli.org/publication/wikp/</link>
      <pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/wikp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture</title>
      <link>https://wenyanli.org/publication/emnlp2024/</link>
      <pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/emnlp2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning</title>
      <link>https://wenyanli.org/publication/acl2024/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/acl2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Role of Data Curation in Image Captioning</title>
      <link>https://wenyanli.org/publication/sd-synth2023/</link>
      <pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/sd-synth2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BLIP models</title>
      <link>https://wenyanli.org/post/blip-notes/</link>
      <pubDate>Mon, 06 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/post/blip-notes/</guid>
      <description>&lt;h1 id=&#34;blips&#34;&gt;BLIPs&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;BLIP&lt;/th&gt;
&lt;th&gt;BLIP2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Image Encoder&lt;/td&gt;
&lt;td&gt;updated&lt;/td&gt;
&lt;td&gt;frozen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Text encoder/decoder&lt;/td&gt;
&lt;td&gt;updated&lt;/td&gt;
&lt;td&gt;frozen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Transformer&lt;/td&gt;
&lt;td&gt;updated （BERT）&lt;/td&gt;
&lt;td&gt;query transformer (Q-Former)， including query embeddings （additional）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pre-training&lt;/td&gt;
&lt;td&gt;pretraining with CapFilt&lt;/td&gt;
&lt;td&gt;2 stage pretraining with CapFilt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;blip&#34;&gt;BLIP&lt;/h2&gt;
&lt;h3 id=&#34;main-method&#34;&gt;Main method:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CapFilt&lt;/code&gt;: generate synthetic captions and filter to get &lt;code&gt;(img,txt)&lt;/code&gt; pairs&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Multimodal mixture of Encoder-Decoder (MED)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;med&#34;&gt;MED&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model structure:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Image encoder&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Text encoder — BERT(Bi SelfAttn, FF)&lt;/p&gt;
&lt;p&gt;这一大块的模型变种共享除了self attention以外的参数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;additional cross attention ⇒ Image-grounded Text Encoder
&lt;ul&gt;
&lt;li&gt;injects visual information with &lt;code&gt;[Encode]&lt;/code&gt; token&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;selfAttn + Casual SelfAttn⇒ Image-grounded Text Decoder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Three vision-language objectives&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;分别与上面text相关模块变种负责的功能对应&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image text contrastive learning (ITC)&lt;/li&gt;
&lt;li&gt;image-text matching (ITM)&lt;/li&gt;
&lt;li&gt;image conditioned language modeling (ITG/LM)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_17.59.49.png&#34; alt=&#34;Screenshot 2023-02-08 at 17.59.49.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;capfilt&#34;&gt;CapFilt&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_18.10.25.png&#34; alt=&#34;Screenshot 2023-02-08 at 18.10.25.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;用COCO训练了一个captioning模型用来生成synthetic captions. Image-grounded text encoder用来过滤所生成的captions。&lt;/p&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_18.21.41.png&#34; alt=&#34;Screenshot 2023-02-08 at 18.21.41.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CapFilt 机制能够比较好地改善模型效果，对比网络抓取的caption，生成的caption效果更好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_18.15.36.png&#34; alt=&#34;Screenshot 2023-02-08 at 18.15.36.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;下游任务在VQA, retrieval, NLVR2, visual dialog以及captioning都做了evaluation。效果均超过baseline， 使用CAPFILT后效果在BLIP base上进一步提升。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;blip2&#34;&gt;BLIP2&lt;/h1&gt;
&lt;h3 id=&#34;main-method-1&#34;&gt;Main method:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Image encoder 和 LLM 都是frozen的，只有&lt;code&gt;querying  transformer&lt;/code&gt; 参数更新。
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Q-former&lt;/code&gt;通过两个阶段的预训练起到一个两个模态间的桥梁作用，且轻量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Q-former&lt;/code&gt; Training involves two stages:
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;bootstrapping image encoder&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;bootstrapping LLM&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data: 129M images (COCO, VG, CC12M, SBU, LAION400M) + BLIP caption + CapFilt&lt;/li&gt;
&lt;li&gt;Submodules:
&lt;ul&gt;
&lt;li&gt;Image encoder:&lt;/li&gt;
&lt;li&gt;LLM: decoder-only OPT; encoder-decoder instruction-trained FlanT5&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;main-contribution&#34;&gt;Main contribution:&lt;/h3&gt;
&lt;p&gt;提出更轻量的VL模型，outperform Flamingo but 54x lighter.&lt;/p&gt;
&lt;h3 id=&#34;problemschallenges-in-previous-models&#34;&gt;Problems/challenges in previous models:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;image-to-text generation loss&lt;/code&gt; are &lt;strong&gt;not sufficient&lt;/strong&gt; to bridge modality gap from pretrained frozen models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;how to do better vision-language alignment? 如何实现多模态预训练模型之间的alignment？&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;heading&#34;&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_14.26.58.png&#34; alt=&#34;Screenshot 2023-02-08 at 14.26.58.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;q-former&#34;&gt;&lt;strong&gt;Q-Former&lt;/strong&gt;:&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_14.53.33.png&#34; alt=&#34;Screenshot 2023-02-08 at 14.53.33.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model structure&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;two transformers:  这两个transformer同时也对应两个阶段的预训练&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image transformer: 从image encoder提取visual feature
&lt;ul&gt;
&lt;li&gt;bottle neck for feature extraction (比直接使用frozen image encoder更轻量）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;text transformer：function as both a text encoder and a text decoder (既可以当encoder用也可以当decoder用）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;inputs: (image, text)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;loss:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;image-text matching  （ITM）— bidirectional&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;image-text contrastive learning （ITC）— unimodal masking&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;image-grounded text generation （ITG）— causal self-attention masking&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_15.06.51.png&#34; alt=&#34;Screenshot 2023-02-08 at 15.06.51.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实现细节&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自注意力层共享参数;&lt;/li&gt;
&lt;li&gt;通过query embeddings和共享的自注意力层实现针对不同人物不同方式的图文交互
&lt;ul&gt;
&lt;li&gt;query embedding (32x768)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;weights initialised from $BERT_{base}$ （188M #parameters）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Learning stages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;representation learning： 学习图像文本的相关性 alignment  (frozen image encoder)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we perform vision-language representation learning which enforces the Q-Former to learn visual representation most relevant to the text.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;generative learning： 有点像prompt- tuning，让学习到的图像feature适配LLM (frozen LLM)&lt;/p&gt;
&lt;p&gt;这个适配直接通过FC层完成。LLM 的输入= &lt;code&gt;concat (FC(visual feature);text_embed)&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we perform vision-to-language generative learning by connecting the output of the Q-Former to a frozen LLM, and trains the Q-Former such that its output visual representation can be interpreted by the LLM.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_15.12.25.png&#34; alt=&#34;Screenshot 2023-02-08 at 15.12.25.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;zero-shot VQA实验效果相当惊艳，秒杀一系列超大模型。Text-encoder部分使用FlanT5效果显著。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_16.17.30.png&#34; alt=&#34;Screenshot 2023-02-08 at 16.17.30.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;I&lt;strong&gt;mage captioning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NoCaps zeroshot performance显著超越baseline，但是在COCO上finetune后的结果并没有提高。这里我认为是因为Image encoder和LLM都是超大模型，如果finetune image encoder反而会减弱它的能力（与之前的Frozen道理相同）。但是不太理解为什么这里作者要去对image encoder进行finetune，也没有给出只finetune Q-Former的对比实验。按道理来说还是一样去finetune Q-Former应该就能得到比较好的效果了。或者直接给出zero-shot结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image-text retrieval&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;retrieval 的实验在BLIP的效果就已经比较饱和了。这里BLIP2在Flickr30k和COCO上都有进一步提升。同样这里BLIP2也是finetune Q-Former 和 image encoder。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ablation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_16.25.15.png&#34; alt=&#34;两阶段预训练效果显著，如果去掉第一部分的预训练，下游任务效果会显著下降。&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;两阶段预训练效果显著，如果去掉第一部分的预训练，下游任务效果会显著下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;Limitation&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“lack of in-context learning”&lt;/li&gt;
&lt;li&gt;“unsatisfactory results due to various reasons including inaccurate knowledge from the LLM, activating the incorrect reasoning path, or not having up-to-date information about new image content”&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MAP: Low-data Regime Multimodal Learning with Adapter-based Pre-training and Prompting</title>
      <link>https://wenyanli.org/publication/lsd2023/</link>
      <pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/lsd2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Systems and Methods for Training Voice Query Models</title>
      <link>https://wenyanli.org/publication/patent_wli_2021/</link>
      <pubDate>Thu, 27 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/patent_wli_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Attentive Recurrent Model for Incremental Prediction of Sentence-final Verbs</title>
      <link>https://wenyanli.org/publication/emnlp2020/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/emnlp2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Auto-annotation for Voice-enabled Entertainment Systems</title>
      <link>https://wenyanli.org/publication/sigir2020/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/publication/sigir2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://wenyanli.org/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Press &lt;span class=&#34;sb&#34;&gt;`S`&lt;/span&gt; key to view
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% /speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/media/boards.jpg&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;#0000FF&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;my-style&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h3&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;navy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RNN review paper notes</title>
      <link>https://wenyanli.org/post/getting-started/</link>
      <pubDate>Wed, 13 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/post/getting-started/</guid>
      <description>&lt;h1 id=&#34;reading-notes-of-a-critical-review-of-recurrent-neural-networks-for-sequence-learninghttpsarxivorgpdf150600019pdf&#34;&gt;Reading notes of &lt;a href=&#34;https://arxiv.org/pdf/1506.00019.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Critical Review of Recurrent Neural Networks for Sequence Learning&lt;/a&gt;.&lt;/h1&gt;
&lt;h2 id=&#34;main-idea&#34;&gt;Main Idea&lt;/h2&gt;
&lt;p&gt;Answered the question of &lt;strong&gt;Why recurrent neural networks?&lt;/strong&gt; in aspects of Architectures, algorithms, results, comparison to alternative methodologies on sequence learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNNs are able to manage text sequences of arbitrary lengths. While processing each token/element in the sequence one at a time, they pass information across time steps.
&lt;ul&gt;
&lt;li&gt;SVM, logistic regression and feedforward networks proved to be useful under independece assumption which precludes modeling long-range dependencies.&lt;/li&gt;
&lt;li&gt;HMM has memeoryless property and has exponentially growing state space even when considering large context window which is computationally impractical for modelling long-range dependencies.&lt;/li&gt;
&lt;li&gt;RNNs are differentiable end to end given fixed architectures and regularization techniques can be used for preventing overfitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;neural-nets-and-rnns&#34;&gt;Neural Nets and RNNs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Neural nets: Neurons in the neural networks has values equal to the output of the activation function applied on the weighted sum of the input nodes&amp;rsquo; values: $ v_{j} = l_{j}(\sum_{j&amp;rsquo;}w_{jj&amp;rsquo;}\cdot{v_{j&amp;rsquo;}})$, which was then written as  $ v_{j} = \sigma(a_j)$
where $a_j = \sum_{j&amp;rsquo;}w_{jj&amp;rsquo;}\cdot{v_{j&amp;rsquo;}}$ and $\sigma$ for sigmoid. And activation functions are task-specific, for example, softmax are normally used in multiclass problems.&lt;/li&gt;
&lt;li&gt;Feedforward and backprop
&lt;ul&gt;
&lt;li&gt;Goal of feedforward NNs: minimize loss function: $L(\hat{y}, y)$.&lt;/li&gt;
&lt;li&gt;Backprop for training: calculate derivatives of the loss function $L$ wrt the parameters of the network. Parameters are then updated by stochastic gradient descent (SGD), i.e. take steps of modifications towards the expecting direction which decreases the loss $L$. Note, loss function may be non-convex, thus global optimal is not gauranteed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RNNs: takes both current input $\mathbf{x}^{(t)}$ and hidden state from previous $\mathbf{h}^{(t-1)}$:
&lt;ul&gt;
&lt;li&gt;$h^{(t)} = \sigma(W^{hx}\mathbf{x}^{(t)}+W^{hh}\mathbf{h}^{(t-1)} + \mathbf{b}_h)$&lt;/li&gt;
&lt;li&gt;$\hat{\mathbf{y}}^{t}= softmax(W^{yh}\mathbf{h}^{(t)} + \mathbf{b}_y)$
With above forward network, backprop of RNN is called Backprop through time (BPTT).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Exploding/vanishing gradients: When learning long-distance dependencies, the gradients of weights may explode or vanish through the time steps. The following tutorial also gives a  detailed description. &lt;a href=&#34;https://arxiv.org/abs/1703.01619&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Machine Translation and Sequence-to-sequence Models: A Tutorial&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lstms-grus-bidirectional&#34;&gt;LSTMs, GRUs, Bidirectional&lt;/h2&gt;
&lt;p&gt;A memory cell and gates are introduced in LSTM, which has unit gradient when adding previous memory $\mathbf{c}_{t-1}$ to the current $\mathbf{c}_t$.
Bidirectional structure has one more layer of hidden nodes which takes inputs from the end, which allows context information before and after the current step.&lt;/p&gt;
&lt;p&gt;Gated recurrent units (GRUs) appear as a simpler and computationally easier alternative of LSTM, which was not introducted in this paper but has on par performance as LSTM.&lt;/p&gt;
&lt;h2 id=&#34;neural-turing-machines&#34;&gt;Neural Turing Machines&lt;/h2&gt;
&lt;p&gt;Incorporate external memory buffer compared to RNNs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Memory matrix +  controller(may be a rnn)&lt;/li&gt;
&lt;li&gt;differentiable end-to-end&lt;/li&gt;
&lt;li&gt;can backprop via SGD&lt;/li&gt;
&lt;li&gt;outperforms LSTM (without external memory) when generalizing on longer inputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;representing natural language inputs and outputs
&lt;ul&gt;
&lt;li&gt;character-level, word level, one-hot, word embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BLEU score for translation evaluation
&lt;ul&gt;
&lt;li&gt;vrevity penalty, ngram precisions&lt;/li&gt;
&lt;li&gt;METEOR as an alternative which is based on word to word matches&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;translation as a SEQ2SEQ problem&lt;/li&gt;
&lt;li&gt;image captioning:  input images and target captions as inputs. Encoding can be dong using CNN for images. Attention mechanism can be used in decoding.&lt;/li&gt;
&lt;li&gt;More: handwriting recognition, unsupervised video encoding, video captioning, computer programs reading.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;discussion&#34;&gt;discussion&lt;/h3&gt;
&lt;p&gt;I don&amp;rsquo;t include most of the equations in the notes. The paper is clear in why and how RNN makes a difference in learning long-dependencies, which serves pretty well as a review/tutorial from a higher level with math well explained.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Phenotype from Genomic Sequence with Deep Neural Networks</title>
      <link>https://wenyanli.org/project/example/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/project/example/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In Yu M. et al. (2016), phenotype is translated from genotype based on gene ontology, and predicted interaction scores
may be influenced by errors in gene annotations or relationship between terms. As deep learning being effective in
identifying complex patterns from feature-rich datasets, especially as recurrent neural networks(RNNs) such as long
short term memory(LSTM) and gated recurrent unit(GRU) are capable of dealing with long-distance sequential data,
predicting genetic interactions directly from DNA or amino-acid sequences using deep learning techniques would help us
gain insights into underlying complex phenotypes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://wenyanli.org/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
