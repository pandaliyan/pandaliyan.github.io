<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Wenyan Li</title>
    <link>https://wenyanli.org/post/</link>
      <atom:link href="https://wenyanli.org/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 06 Nov 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wenyanli.org/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://wenyanli.org/post/</link>
    </image>
    
    <item>
      <title>BLIP models</title>
      <link>https://wenyanli.org/post/blip-notes/</link>
      <pubDate>Mon, 06 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/post/blip-notes/</guid>
      <description>&lt;h1 id=&#34;blips&#34;&gt;BLIPs&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;BLIP&lt;/th&gt;
&lt;th&gt;BLIP2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Image Encoder&lt;/td&gt;
&lt;td&gt;updated&lt;/td&gt;
&lt;td&gt;frozen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Text encoder/decoder&lt;/td&gt;
&lt;td&gt;updated&lt;/td&gt;
&lt;td&gt;frozen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Transformer&lt;/td&gt;
&lt;td&gt;updated （BERT）&lt;/td&gt;
&lt;td&gt;query transformer (Q-Former)， including query embeddings （additional）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pre-training&lt;/td&gt;
&lt;td&gt;pretraining with CapFilt&lt;/td&gt;
&lt;td&gt;2 stage pretraining with CapFilt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;blip&#34;&gt;BLIP&lt;/h2&gt;
&lt;h3 id=&#34;main-method&#34;&gt;Main method:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CapFilt&lt;/code&gt;: generate synthetic captions and filter to get &lt;code&gt;(img,txt)&lt;/code&gt; pairs&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Multimodal mixture of Encoder-Decoder (MED)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;med&#34;&gt;MED&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model structure:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Image encoder&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Text encoder — BERT(Bi SelfAttn, FF)&lt;/p&gt;
&lt;p&gt;这一大块的模型变种共享除了self attention以外的参数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;additional cross attention ⇒ Image-grounded Text Encoder
&lt;ul&gt;
&lt;li&gt;injects visual information with &lt;code&gt;[Encode]&lt;/code&gt; token&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;selfAttn + Casual SelfAttn⇒ Image-grounded Text Decoder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Three vision-language objectives&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;分别与上面text相关模块变种负责的功能对应&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image text contrastive learning (ITC)&lt;/li&gt;
&lt;li&gt;image-text matching (ITM)&lt;/li&gt;
&lt;li&gt;image conditioned language modeling (ITG/LM)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_17.59.49.png&#34; alt=&#34;Screenshot 2023-02-08 at 17.59.49.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;capfilt&#34;&gt;CapFilt&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_18.10.25.png&#34; alt=&#34;Screenshot 2023-02-08 at 18.10.25.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;用COCO训练了一个captioning模型用来生成synthetic captions. Image-grounded text encoder用来过滤所生成的captions。&lt;/p&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_18.21.41.png&#34; alt=&#34;Screenshot 2023-02-08 at 18.21.41.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CapFilt 机制能够比较好地改善模型效果，对比网络抓取的caption，生成的caption效果更好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_18.15.36.png&#34; alt=&#34;Screenshot 2023-02-08 at 18.15.36.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;下游任务在VQA, retrieval, NLVR2, visual dialog以及captioning都做了evaluation。效果均超过baseline， 使用CAPFILT后效果在BLIP base上进一步提升。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;blip2&#34;&gt;BLIP2&lt;/h1&gt;
&lt;h3 id=&#34;main-method-1&#34;&gt;Main method:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Image encoder 和 LLM 都是frozen的，只有&lt;code&gt;querying  transformer&lt;/code&gt; 参数更新。
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Q-former&lt;/code&gt;通过两个阶段的预训练起到一个两个模态间的桥梁作用，且轻量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Q-former&lt;/code&gt; Training involves two stages:
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;bootstrapping image encoder&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;bootstrapping LLM&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data: 129M images (COCO, VG, CC12M, SBU, LAION400M) + BLIP caption + CapFilt&lt;/li&gt;
&lt;li&gt;Submodules:
&lt;ul&gt;
&lt;li&gt;Image encoder:&lt;/li&gt;
&lt;li&gt;LLM: decoder-only OPT; encoder-decoder instruction-trained FlanT5&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;main-contribution&#34;&gt;Main contribution:&lt;/h3&gt;
&lt;p&gt;提出更轻量的VL模型，outperform Flamingo but 54x lighter.&lt;/p&gt;
&lt;h3 id=&#34;problemschallenges-in-previous-models&#34;&gt;Problems/challenges in previous models:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;image-to-text generation loss&lt;/code&gt; are &lt;strong&gt;not sufficient&lt;/strong&gt; to bridge modality gap from pretrained frozen models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;how to do better vision-language alignment? 如何实现多模态预训练模型之间的alignment？&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;heading&#34;&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_14.26.58.png&#34; alt=&#34;Screenshot 2023-02-08 at 14.26.58.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;q-former&#34;&gt;&lt;strong&gt;Q-Former&lt;/strong&gt;:&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_14.53.33.png&#34; alt=&#34;Screenshot 2023-02-08 at 14.53.33.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model structure&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;two transformers:  这两个transformer同时也对应两个阶段的预训练&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image transformer: 从image encoder提取visual feature
&lt;ul&gt;
&lt;li&gt;bottle neck for feature extraction (比直接使用frozen image encoder更轻量）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;text transformer：function as both a text encoder and a text decoder (既可以当encoder用也可以当decoder用）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;inputs: (image, text)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;loss:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;image-text matching  （ITM）— bidirectional&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;image-text contrastive learning （ITC）— unimodal masking&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;image-grounded text generation （ITG）— causal self-attention masking&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_15.06.51.png&#34; alt=&#34;Screenshot 2023-02-08 at 15.06.51.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实现细节&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自注意力层共享参数;&lt;/li&gt;
&lt;li&gt;通过query embeddings和共享的自注意力层实现针对不同人物不同方式的图文交互
&lt;ul&gt;
&lt;li&gt;query embedding (32x768)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;weights initialised from $BERT_{base}$ （188M #parameters）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Learning stages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;representation learning： 学习图像文本的相关性 alignment  (frozen image encoder)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we perform vision-language representation learning which enforces the Q-Former to learn visual representation most relevant to the text.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;generative learning： 有点像prompt- tuning，让学习到的图像feature适配LLM (frozen LLM)&lt;/p&gt;
&lt;p&gt;这个适配直接通过FC层完成。LLM 的输入= &lt;code&gt;concat (FC(visual feature);text_embed)&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we perform vision-to-language generative learning by connecting the output of the Q-Former to a frozen LLM, and trains the Q-Former such that its output visual representation can be interpreted by the LLM.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_15.12.25.png&#34; alt=&#34;Screenshot 2023-02-08 at 15.12.25.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;zero-shot VQA实验效果相当惊艳，秒杀一系列超大模型。Text-encoder部分使用FlanT5效果显著。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_16.17.30.png&#34; alt=&#34;Screenshot 2023-02-08 at 16.17.30.png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;I&lt;strong&gt;mage captioning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NoCaps zeroshot performance显著超越baseline，但是在COCO上finetune后的结果并没有提高。这里我认为是因为Image encoder和LLM都是超大模型，如果finetune image encoder反而会减弱它的能力（与之前的Frozen道理相同）。但是不太理解为什么这里作者要去对image encoder进行finetune，也没有给出只finetune Q-Former的对比实验。按道理来说还是一样去finetune Q-Former应该就能得到比较好的效果了。或者直接给出zero-shot结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image-text retrieval&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;retrieval 的实验在BLIP的效果就已经比较饱和了。这里BLIP2在Flickr30k和COCO上都有进一步提升。同样这里BLIP2也是finetune Q-Former 和 image encoder。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ablation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;BLIPs%20a55ad040f589466db86136a738c9cfa2/Screenshot_2023-02-08_at_16.25.15.png&#34; alt=&#34;两阶段预训练效果显著，如果去掉第一部分的预训练，下游任务效果会显著下降。&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;两阶段预训练效果显著，如果去掉第一部分的预训练，下游任务效果会显著下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;Limitation&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“lack of in-context learning”&lt;/li&gt;
&lt;li&gt;“unsatisfactory results due to various reasons including inaccurate knowledge from the LLM, activating the incorrect reasoning path, or not having up-to-date information about new image content”&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>RNN review paper notes</title>
      <link>https://wenyanli.org/post/getting-started/</link>
      <pubDate>Wed, 13 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://wenyanli.org/post/getting-started/</guid>
      <description>&lt;h1 id=&#34;reading-notes-of-a-critical-review-of-recurrent-neural-networks-for-sequence-learninghttpsarxivorgpdf150600019pdf&#34;&gt;Reading notes of &lt;a href=&#34;https://arxiv.org/pdf/1506.00019.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Critical Review of Recurrent Neural Networks for Sequence Learning&lt;/a&gt;.&lt;/h1&gt;
&lt;h2 id=&#34;main-idea&#34;&gt;Main Idea&lt;/h2&gt;
&lt;p&gt;Answered the question of &lt;strong&gt;Why recurrent neural networks?&lt;/strong&gt; in aspects of Architectures, algorithms, results, comparison to alternative methodologies on sequence learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNNs are able to manage text sequences of arbitrary lengths. While processing each token/element in the sequence one at a time, they pass information across time steps.
&lt;ul&gt;
&lt;li&gt;SVM, logistic regression and feedforward networks proved to be useful under independece assumption which precludes modeling long-range dependencies.&lt;/li&gt;
&lt;li&gt;HMM has memeoryless property and has exponentially growing state space even when considering large context window which is computationally impractical for modelling long-range dependencies.&lt;/li&gt;
&lt;li&gt;RNNs are differentiable end to end given fixed architectures and regularization techniques can be used for preventing overfitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;neural-nets-and-rnns&#34;&gt;Neural Nets and RNNs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Neural nets: Neurons in the neural networks has values equal to the output of the activation function applied on the weighted sum of the input nodes&amp;rsquo; values: $ v_{j} = l_{j}(\sum_{j&amp;rsquo;}w_{jj&amp;rsquo;}\cdot{v_{j&amp;rsquo;}})$, which was then written as  $ v_{j} = \sigma(a_j)$
where $a_j = \sum_{j&amp;rsquo;}w_{jj&amp;rsquo;}\cdot{v_{j&amp;rsquo;}}$ and $\sigma$ for sigmoid. And activation functions are task-specific, for example, softmax are normally used in multiclass problems.&lt;/li&gt;
&lt;li&gt;Feedforward and backprop
&lt;ul&gt;
&lt;li&gt;Goal of feedforward NNs: minimize loss function: $L(\hat{y}, y)$.&lt;/li&gt;
&lt;li&gt;Backprop for training: calculate derivatives of the loss function $L$ wrt the parameters of the network. Parameters are then updated by stochastic gradient descent (SGD), i.e. take steps of modifications towards the expecting direction which decreases the loss $L$. Note, loss function may be non-convex, thus global optimal is not gauranteed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RNNs: takes both current input $\mathbf{x}^{(t)}$ and hidden state from previous $\mathbf{h}^{(t-1)}$:
&lt;ul&gt;
&lt;li&gt;$h^{(t)} = \sigma(W^{hx}\mathbf{x}^{(t)}+W^{hh}\mathbf{h}^{(t-1)} + \mathbf{b}_h)$&lt;/li&gt;
&lt;li&gt;$\hat{\mathbf{y}}^{t}= softmax(W^{yh}\mathbf{h}^{(t)} + \mathbf{b}_y)$
With above forward network, backprop of RNN is called Backprop through time (BPTT).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Exploding/vanishing gradients: When learning long-distance dependencies, the gradients of weights may explode or vanish through the time steps. The following tutorial also gives a  detailed description. &lt;a href=&#34;https://arxiv.org/abs/1703.01619&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Machine Translation and Sequence-to-sequence Models: A Tutorial&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lstms-grus-bidirectional&#34;&gt;LSTMs, GRUs, Bidirectional&lt;/h2&gt;
&lt;p&gt;A memory cell and gates are introduced in LSTM, which has unit gradient when adding previous memory $\mathbf{c}_{t-1}$ to the current $\mathbf{c}_t$.
Bidirectional structure has one more layer of hidden nodes which takes inputs from the end, which allows context information before and after the current step.&lt;/p&gt;
&lt;p&gt;Gated recurrent units (GRUs) appear as a simpler and computationally easier alternative of LSTM, which was not introducted in this paper but has on par performance as LSTM.&lt;/p&gt;
&lt;h2 id=&#34;neural-turing-machines&#34;&gt;Neural Turing Machines&lt;/h2&gt;
&lt;p&gt;Incorporate external memory buffer compared to RNNs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Memory matrix +  controller(may be a rnn)&lt;/li&gt;
&lt;li&gt;differentiable end-to-end&lt;/li&gt;
&lt;li&gt;can backprop via SGD&lt;/li&gt;
&lt;li&gt;outperforms LSTM (without external memory) when generalizing on longer inputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;representing natural language inputs and outputs
&lt;ul&gt;
&lt;li&gt;character-level, word level, one-hot, word embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BLEU score for translation evaluation
&lt;ul&gt;
&lt;li&gt;vrevity penalty, ngram precisions&lt;/li&gt;
&lt;li&gt;METEOR as an alternative which is based on word to word matches&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;translation as a SEQ2SEQ problem&lt;/li&gt;
&lt;li&gt;image captioning:  input images and target captions as inputs. Encoding can be dong using CNN for images. Attention mechanism can be used in decoding.&lt;/li&gt;
&lt;li&gt;More: handwriting recognition, unsupervised video encoding, video captioning, computer programs reading.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;discussion&#34;&gt;discussion&lt;/h3&gt;
&lt;p&gt;I don&amp;rsquo;t include most of the equations in the notes. The paper is clear in why and how RNN makes a difference in learning long-dependencies, which serves pretty well as a review/tutorial from a higher level with math well explained.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
