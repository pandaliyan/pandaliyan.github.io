[{"authors":null,"categories":null,"content":"Hi! I am Wenyan, a PhD student at the CoAStaL NLP Group, University of Copenhagen. I’m supervised by Anders Søgaard.\nBefore this, I worked as a Senior NLP Researcher at Sensetime and Comcast AI Research Lab (mentored by Ferhan Ture).\nI completed my MS at University of Maryland, College Park, where I worked with Prof. Jordan Boyd-Graber on Natural Language Processing.\nIn my free time, I enjoy painting, cooking, yoga, and table tennis :)\nNEWS: «««\u0026lt; HEAD\n 11/2024 – Invited talk and shor visit at MIT! 11/2024 – Our W1KP paper won the Outstanding Paper Award at EMNLP 2024! 11/2024 – I will present FoodieQA in EMNLP 2024, see you in Miami! 09/2024 – FoodieQA and W1KP are accepted to EMNLP 2024 main conference! 05/2024 – One paper accepted to ACL 2024 main conference!  ","date":1731715200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1731715200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi! I am Wenyan, a PhD student at the CoAStaL NLP Group, University of Copenhagen. I’m supervised by Anders Søgaard.\nBefore this, I worked as a Senior NLP Researcher at Sensetime and Comcast AI Research Lab (mentored by Ferhan Ture).","tags":null,"title":"Wenyan Li","type":"authors"},{"authors":["Raphael Tang","Crystina Zhang","Lixinyu Xu","Yao Lu","Wenyan Li","Pontus Stenetorp","Jimmy Lin","Ferhan Ture"],"categories":null,"content":"","date":1731715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731715200,"objectID":"261e217ca8e546d3e737edefeecf2a68","permalink":"https://wenyanli.org/publication/wikp/","publishdate":"2024-11-16T00:00:00Z","relpermalink":"/publication/wikp/","section":"publication","summary":"Diffusion models are the state of the art in text-to-image generation, but their perceptual variability remains understudied. In this paper, we examine how prompts affect image variability in black-box diffusion-based models. We propose W1KP, a human-calibrated measure of variability in a set of images, bootstrapped from existing image-pair perceptual distances. Current datasets do not cover recent diffusion models, thus we curate three test sets for evaluation. Our best perceptual distance outperforms nine baselines by up to 18 points in accuracy, and our calibration matches graded human judgements 78% of the time. Using W1KP, we study prompt reusability and show that Imagen prompts can be reused for 10-50 random seeds before new images become too similar to already generated images, while Stable Diffusion XL and DALL-E 3 can be reused 50-200 times. Lastly, we analyze 56 linguistic features of real prompts, finding that the prompt's length, CLIP embedding norm, concreteness, and word senses influence variability most. As far as we are aware, we are the first to analyze diffusion variability from a visuolinguistic perspective.","tags":["Multimodal"],"title":"Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation","type":"publication"},{"authors":["Wenyan Li","Xinyu Zhang","Jiaang Li","Qiwei Peng","Raphael Tang","Li Zhou","Weijia Zhang","Guimin Hu","Yifei Yuan","Anders Søgaard","Daniel Hershcovich","Desmond Elliott"],"categories":null,"content":"","date":1718496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718496000,"objectID":"fbe0afa103823fa08c067e66ebb1d53f","permalink":"https://wenyanli.org/publication/emnlp2024/","publishdate":"2024-06-16T00:00:00Z","relpermalink":"/publication/emnlp2024/","section":"publication","summary":"Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision-language Models (VLMs) and large language models (LLMs) on newly collected, unseen food images and corresponding questions. FoodieQA comprises three multiple-choice question-answering tasks where models need to answer questions based on multiple images, a single image, and text-only descriptions, respectively.","tags":["Multimodal"],"title":"FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture","type":"publication"},{"authors":["Wenyan Li","Jiaang Li","Rita Ramos","Raphael Tang","Desmond Elliott"],"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"ed9e5b3960823d731945192184443263","permalink":"https://wenyanli.org/publication/acl2024/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/publication/acl2024/","section":"publication","summary":"Recent advances in retrieval-augmented models for image captioning highlight the benefit of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice: the retrieved information can sometimes mislead the model, resulting in incorrect generation and worse performance. In this paper, we analyze the robustness of a retrieval-augmented captioning model SmallCap. Our analysis shows that the model is sensitive to tokens that appear in the majority of the retrieved captions, and the input attribution shows that those tokens are likely copied into the generated output. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This decreases the chance that the model learns to copy majority tokens, and improves both in-domain and cross-domain performance.","tags":["Multimodal"],"title":"Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning","type":"publication"},{"authors":["Wenyan Li","Jonas F Lotz","Chen Qiu","Desmond Elliott"],"categories":null,"content":"","date":1705622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705622400,"objectID":"b1ac385971b6b1e891a8dcb9f9e758a4","permalink":"https://wenyanli.org/publication/sd-synth2023/","publishdate":"2024-01-19T00:00:00Z","relpermalink":"/publication/sd-synth2023/","section":"publication","summary":"Image captioning models are typically trained by treating all samples equally, neglecting to account for mismatched or otherwise difficult data points. In contrast, recent work has shown the effectiveness of training models by scheduling the data using curriculum learning strategies. This paper contributes to this direction by actively curating difficult samples in datasets *without* increasing the total number of samples. We explore the effect of using three data curation methods within the training process: complete removal of an sample, caption replacement, or image replacement via a text-to-image generation model. Experiments on the Flickr30K and COCO datasets with the BLIP and BEiT-3 models demonstrate that these curation methods do indeed yield improved image captioning models, underscoring their efficacy.","tags":["Multimodal"],"title":"The Role of Data Curation in Image Captioning","type":"publication"},{"authors":["Wenyan Li"],"categories":["reading"],"content":"BLIPs     BLIP BLIP2     Image Encoder updated frozen   Text encoder/decoder updated frozen   Transformer updated （BERT） query transformer (Q-Former)， including query embeddings （additional）   pre-training pretraining with CapFilt 2 stage pretraining with CapFilt    BLIP Main method:  CapFilt: generate synthetic captions and filter to get (img,txt) pairs Multimodal mixture of Encoder-Decoder (MED)  MED Model structure:\n  Image encoder\n  Text encoder — BERT(Bi SelfAttn, FF)\n这一大块的模型变种共享除了self attention以外的参数\n   additional cross attention ⇒ Image-grounded Text Encoder  injects visual information with [Encode] token       selfAttn + Casual SelfAttn⇒ Image-grounded Text Decoder      Three vision-language objectives:\n分别与上面text相关模块变种负责的功能对应\n image text contrastive learning (ITC) image-text matching (ITM) image conditioned language modeling (ITG/LM)     CapFilt    用COCO训练了一个captioning模型用来生成synthetic captions. Image-grounded text encoder用来过滤所生成的captions。\nExperiments     CapFilt 机制能够比较好地改善模型效果，对比网络抓取的caption，生成的caption效果更好。     下游任务在VQA, retrieval, NLVR2, visual dialog以及captioning都做了evaluation。效果均超过baseline， 使用CAPFILT后效果在BLIP base上进一步提升。\n BLIP2 Main method:  Image encoder 和 LLM 都是frozen的，只有querying transformer 参数更新。  Q-former通过两个阶段的预训练起到一个两个模态间的桥梁作用，且轻量   Q-former Training involves two stages:    bootstrapping image encoder    bootstrapping LLM     Data: 129M images (COCO, VG, CC12M, SBU, LAION400M) + BLIP caption + CapFilt Submodules:  Image encoder: LLM: decoder-only OPT; encoder-decoder instruction-trained FlanT5    Main contribution: 提出更轻量的VL模型，outperform Flamingo but 54x lighter.\nProblems/challenges in previous models:  image-to-text generation loss are not sufficient to bridge modality gap from pretrained frozen models. how to do better vision-language alignment? 如何实现多模态预训练模型之间的alignment？      Q-Former:    Model structure\n  two transformers: 这两个transformer同时也对应两个阶段的预训练\n image transformer: 从image encoder提取visual feature  bottle neck for feature extraction (比直接使用frozen image encoder更轻量）   text transformer：function as both a text encoder and a text decoder (既可以当encoder用也可以当decoder用）    inputs: (image, text)\n  loss:\n   image-text matching （ITM）— bidirectional    image-text contrastive learning （ITC）— unimodal masking    image-grounded text generation （ITG）— causal self-attention masking         实现细节\n 自注意力层共享参数; 通过query embeddings和共享的自注意力层实现针对不同人物不同方式的图文交互  query embedding (32x768)   weights initialised from $BERT_{base}$ （188M #parameters）    Learning stages\n  representation learning： 学习图像文本的相关性 alignment (frozen image encoder)\n we perform vision-language representation learning which enforces the Q-Former to learn visual representation most relevant to the text.\n   generative learning： 有点像prompt- tuning，让学习到的图像feature适配LLM (frozen LLM)\n这个适配直接通过FC层完成。LLM 的输入= concat (FC(visual feature);text_embed)\n we perform vision-to-language generative learning by connecting the output of the Q-Former to a frozen LLM, and trains the Q-Former such that its output visual representation can be interpreted by the LLM.\n      Experiments\nzero-shot VQA实验效果相当惊艳，秒杀一系列超大模型。Text-encoder部分使用FlanT5效果显著。\n   Image captioning\nNoCaps zeroshot performance显著超越baseline，但是在COCO上finetune后的结果并没有提高。这里我认为是因为Image encoder和LLM都是超大模型，如果finetune image encoder反而会减弱它的能力（与之前的Frozen道理相同）。但是不太理解为什么这里作者要去对image encoder进行finetune，也没有给出只finetune Q-Former的对比实验。按道理来说还是一样去finetune Q-Former应该就能得到比较好的效果了。或者直接给出zero-shot结果。\nImage-text retrieval\nretrieval 的实验在BLIP的效果就已经比较饱和了。这里BLIP2在Flickr30k和COCO上都有进一步提升。同样这里BLIP2也是finetune Q-Former 和 image encoder。\nAblation\n   两阶段预训练效果显著，如果去掉第一部分的预训练，下游任务效果会显著下降。\nLimitation\n “lack of in-context learning” “unsatisfactory results due to various reasons including inaccurate knowledge from the LLM, activating the incorrect reasoning path, or not having up-to-date information about new image content”  ","date":1699228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699228800,"objectID":"ff88e1e50c9008bb5ca648cd6fffdb36","permalink":"https://wenyanli.org/post/blip-notes/","publishdate":"2023-11-06T00:00:00Z","relpermalink":"/post/blip-notes/","section":"post","summary":"Notes on BLIP models","tags":["reading-notes"],"title":"BLIP models","type":"post"},{"authors":["Wenyan Li","Dong Li","Wanjing Li","Yuanjie Wang","Hai Jie","Yiran Zhong"],"categories":null,"content":"","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693526400,"objectID":"e8e4a21d1a625abcc620199b6c2ebfa3","permalink":"https://wenyanli.org/publication/lsd2023/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/lsd2023/","section":"publication","summary":"Pretrained vision-language (VL) models have shown impressive results on various multi-modal downstream tasks recently. Many of the benchmark models build on pretrained causal language models (LMs), leveraging the original few-shot learning and generalization capability of the LMs trained with large text corpora. However, these models are often gigantic and require large-scale image and text data with high computational cost to train. This paper introduces a moderate-size model called MAP for efficient VL transfer learning through adapter-based pretraining and prompting. We aim to answer the question of how much we can complete through VL pretraining within the low-data regime while maximizing efficiency in transferring knowledge of a moderate-size frozen LM. Our experiments demonstrate that MAP achieves substantially better zero-shot and few-shot performance on downstream VL tasks with only 10% the size of pretraining data and a 30x lighter pretrained LM backbone compared to Frozen. MAP also outperforms fully trained models of comparable size at retaining its transfer learning ability when the amount of training data reduces.","tags":["Multimodal"],"title":"MAP: Low-data Regime Multimodal Learning with Adapter-based Pre-training and Prompting","type":"publication"},{"authors":["Wenyan Li","Ferhan Ture","Jose Casillas","Tom Des Jardins"],"categories":null,"content":"","date":1643241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643241600,"objectID":"6a4d47b3657d243f66812a75918c2674","permalink":"https://wenyanli.org/publication/patent_wli_2021/","publishdate":"2022-01-27T00:00:00Z","relpermalink":"/publication/patent_wli_2021/","section":"publication","summary":"","tags":["NLP","SR"],"title":"Systems and Methods for Training Voice Query Models","type":"publication"},{"authors":["Wenyan Li","Alvin Grissom II","Jordan Boyd-Graber"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"782393386ce24179ff33237b8017667f","permalink":"https://wenyanli.org/publication/emnlp2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/emnlp2020/","section":"publication","summary":"Verb prediction is important for understanding human processing of verb-final languages, with practical applications to real-time simultaneous interpretation from verb-final to verb-medial languages. While previous approaches use classical statistical models, we introduce an attention-based neural model to incrementally predict final verbs on incomplete sentences in Japanese and German SOV sentences. To offer flexibility to the model, we further incorporate synonym awareness. Our approach both better predicts the final verbs in Japanese and German and provides more interpretable explanations of why those verbs are selected.","tags":["NLP"],"title":"An Attentive Recurrent Model for Incremental Prediction of Sentence-final Verbs","type":"publication"},{"authors":["Wenyan Li","Ferhan Ture"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8bebee5048fed72e88390e6f7177b7a1","permalink":"https://wenyanli.org/publication/sigir2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/sigir2020/","section":"publication","summary":"Voice-activated intelligent entertainment systems are prevalent in modern TVs. These systems require accurate automatic speech recognition (ASR) models to transcribe voice queries for further downstream language understanding tasks. Currently, labeling audio data for training is the main bottleneck in deploying accurate machine learning ASR models, especially when these models require up-to-date training data to adapt to the shifting customer needs. We present an auto-annotation system, which provides high quality training data without any hand-labeled audios by detecting speech recognition errors and providing possible fixes. Through our algorithm, the auto-annotated training data reaches an overall word error rate (WER) of 0.002; furthermore, we obtained a reduction of 0.907 in WER after applying the auto-suggested fixes.","tags":["NLP","SR","IR"],"title":"Auto-annotation for Voice-enabled Entertainment Systems","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  Two  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://wenyanli.org/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Wenyan Li"],"categories":["reading"],"content":"Reading notes of A Critical Review of Recurrent Neural Networks for Sequence Learning. Main Idea Answered the question of Why recurrent neural networks? in aspects of Architectures, algorithms, results, comparison to alternative methodologies on sequence learning.\n RNNs are able to manage text sequences of arbitrary lengths. While processing each token/element in the sequence one at a time, they pass information across time steps.  SVM, logistic regression and feedforward networks proved to be useful under independece assumption which precludes modeling long-range dependencies. HMM has memeoryless property and has exponentially growing state space even when considering large context window which is computationally impractical for modelling long-range dependencies. RNNs are differentiable end to end given fixed architectures and regularization techniques can be used for preventing overfitting.    Neural Nets and RNNs  Neural nets: Neurons in the neural networks has values equal to the output of the activation function applied on the weighted sum of the input nodes’ values: $ v_{j} = l_{j}(\\sum_{j’}w_{jj’}\\cdot{v_{j’}})$, which was then written as $ v_{j} = \\sigma(a_j)$ where $a_j = \\sum_{j’}w_{jj’}\\cdot{v_{j’}}$ and $\\sigma$ for sigmoid. And activation functions are task-specific, for example, softmax are normally used in multiclass problems. Feedforward and backprop  Goal of feedforward NNs: minimize loss function: $L(\\hat{y}, y)$. Backprop for training: calculate derivatives of the loss function $L$ wrt the parameters of the network. Parameters are then updated by stochastic gradient descent (SGD), i.e. take steps of modifications towards the expecting direction which decreases the loss $L$. Note, loss function may be non-convex, thus global optimal is not gauranteed   RNNs: takes both current input $\\mathbf{x}^{(t)}$ and hidden state from previous $\\mathbf{h}^{(t-1)}$:  $h^{(t)} = \\sigma(W^{hx}\\mathbf{x}^{(t)}+W^{hh}\\mathbf{h}^{(t-1)} + \\mathbf{b}_h)$ $\\hat{\\mathbf{y}}^{t}= softmax(W^{yh}\\mathbf{h}^{(t)} + \\mathbf{b}_y)$ With above forward network, backprop of RNN is called Backprop through time (BPTT).   Exploding/vanishing gradients: When learning long-distance dependencies, the gradients of weights may explode or vanish through the time steps. The following tutorial also gives a detailed description. Neural Machine Translation and Sequence-to-sequence Models: A Tutorial.  LSTMs, GRUs, Bidirectional A memory cell and gates are introduced in LSTM, which has unit gradient when adding previous memory $\\mathbf{c}_{t-1}$ to the current $\\mathbf{c}_t$. Bidirectional structure has one more layer of hidden nodes which takes inputs from the end, which allows context information before and after the current step.\nGated recurrent units (GRUs) appear as a simpler and computationally easier alternative of LSTM, which was not introducted in this paper but has on par performance as LSTM.\nNeural Turing Machines Incorporate external memory buffer compared to RNNs.\n Memory matrix + controller(may be a rnn) differentiable end-to-end can backprop via SGD outperforms LSTM (without external memory) when generalizing on longer inputs.  Applications  representing natural language inputs and outputs  character-level, word level, one-hot, word embeddings   BLEU score for translation evaluation  vrevity penalty, ngram precisions METEOR as an alternative which is based on word to word matches   translation as a SEQ2SEQ problem image captioning: input images and target captions as inputs. Encoding can be dong using CNN for images. Attention mechanism can be used in decoding. More: handwriting recognition, unsupervised video encoding, video captioning, computer programs reading.  discussion I don’t include most of the equations in the notes. The paper is clear in why and how RNN makes a difference in learning long-dependencies, which serves pretty well as a review/tutorial from a higher level with math well explained.\n","date":1528848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://wenyanli.org/post/getting-started/","publishdate":"2018-06-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Notes on the paper: A Critical Review of Recurrent Neural Networks for Sequence Learning (Lipton and Berkowitz) - about RNN architectures, algorithms, results, comparison to alternative methodologies on sequence learning.","tags":["reading-notes"],"title":"RNN review paper notes","type":"post"},{"authors":null,"categories":null,"content":"Summary In Yu M. et al. (2016), phenotype is translated from genotype based on gene ontology, and predicted interaction scores may be influenced by errors in gene annotations or relationship between terms. As deep learning being effective in identifying complex patterns from feature-rich datasets, especially as recurrent neural networks(RNNs) such as long short term memory(LSTM) and gated recurrent unit(GRU) are capable of dealing with long-distance sequential data, predicting genetic interactions directly from DNA or amino-acid sequences using deep learning techniques would help us gain insights into underlying complex phenotypes.\n","date":1517011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517011200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://wenyanli.org/project/example/","publishdate":"2018-01-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"Predicting pairwise gene interactions using attention-based RNN/CNN, and compared to baseline approach with random forest procedure with gene ontotype described in [Yu, et al. (Cell Systems, 2016)](http://www.cell.com/cell-systems/abstract/S2405-4712(16)30033-3) on the *S. cerevisiae* data from [Costanzo, et al. (Science, 2010)](http://science.sciencemag.org/content/327/5964/425.long).","tags":["Deep Learning"],"title":"Predicting Phenotype from Genomic Sequence with Deep Neural Networks","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://wenyanli.org/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]