[{"authors":null,"categories":null,"content":"Hi! I am Wenyan, a Senior Machine Learning Research Engineer at Comcast AI Research Lab.\nBefore this, I completed my MS at University of Maryland, College Park, where I worked with Prof. Jordan Boyd-Graber on Natural Language Processing. Download my resum√©.\n","date":1528848000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi! I am Wenyan, a Senior Machine Learning Research Engineer at Comcast AI Research Lab.\nBefore this, I completed my MS at University of Maryland, College Park, where I worked with Prof.","tags":null,"title":"Wenyan Li","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://wenyanli.org/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["*Wenyan Li*","Alvin Grissom II","Jordan Boyd-Graber"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"782393386ce24179ff33237b8017667f","permalink":"https://wenyanli.org/publication/emnlp2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/emnlp2020/","section":"publication","summary":"Verb prediction is important for understanding human processing of verb-final languages, with practical applications to real-time simultaneous interpretation from verb-final to verb-medial languages. While previous approaches use classical statistical models, we introduce an attention-based neural model to incrementally predict final verbs on incomplete sentences in Japanese and German SOV sentences. To offer flexibility to the model, we further incorporate synonym awareness. Our approach both better predicts the final verbs in Japanese and German and provides more interpretable explanations of why those verbs are selected.","tags":["papers"],"title":"An Attentive Recurrent Model for Incremental Prediction of Sentence-final Verbs","type":"publication"},{"authors":["Wenyan Li","Ferhan Ture"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8bebee5048fed72e88390e6f7177b7a1","permalink":"https://wenyanli.org/publication/sigir2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/sigir2020/","section":"publication","summary":"Voice-activated intelligent entertainment systems are prevalent in modern TVs. These systems require accurate automatic speech recognition (ASR) models to transcribe voice queries for further downstream language understanding tasks. Currently, labeling audio data for training is the main bottleneck in deploying accurate machine learning ASR models, especially when these models require up-to-date training data to adapt to the shifting customer needs. We present an auto-annotation system, which provides high quality training data without any hand-labeled audios by detecting speech recognition errors and providing possible fixes. Through our algorithm, the auto-annotated training data reaches an overall word error rate (WER) of 0.002; furthermore, we obtained a reduction of 0.907 in WER after applying the auto-suggested fixes.","tags":["papers"],"title":"Auto-annotation for Voice-enabled Entertainment Systems","type":"publication"},{"authors":["Wenyan Li","Ferhan Ture","Jose Casillas","Tom Des Jardins"],"categories":null,"content":"","date":1592870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592870400,"objectID":"6a4d47b3657d243f66812a75918c2674","permalink":"https://wenyanli.org/publication/patent_wli_2021/","publishdate":"2020-06-23T00:00:00Z","relpermalink":"/publication/patent_wli_2021/","section":"publication","summary":"","tags":["patent"],"title":"Systems and Methods for Training Voice Query Models","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://wenyanli.org/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Wenyan Li"],"categories":["reading"],"content":"Reading notes of A Critical Review of Recurrent Neural Networks for Sequence Learning. Main Idea Answered the question of Why recurrent neural networks? in aspects of Architectures, algorithms, results, comparison to alternative methodologies on sequence learning.\n RNNs are able to manage text sequences of arbitrary lengths. While processing each token/element in the sequence one at a time, they pass information across time steps.  SVM, logistic regression and feedforward networks proved to be useful under independece assumption which precludes modeling long-range dependencies. HMM has memeoryless property and has exponentially growing state space even when considering large context window which is computationally impractical for modelling long-range dependencies. RNNs are differentiable end to end given fixed architectures and regularization techniques can be used for preventing overfitting.    Neural Nets and RNNs  Neural nets: Neurons in the neural networks has values equal to the output of the activation function applied on the weighted sum of the input nodes' values: $ v_{j} = l_{j}(\\sum_{j'}w_{jj'}\\cdot{v_{j'}})$, which was then written as $ v_{j} = \\sigma(a_j)$ where $a_j = \\sum_{j'}w_{jj'}\\cdot{v_{j'}}$ and $\\sigma$ for sigmoid. And activation functions are task-specific, for example, softmax are normally used in multiclass problems. Feedforward and backprop  Goal of feedforward NNs: minimize loss function: $L(\\hat{y}, y)$. Backprop for training: calculate derivatives of the loss function $L$ wrt the parameters of the network. Parameters are then updated by stochastic gradient descent (SGD), i.e. take steps of modifications towards the expecting direction which decreases the loss $L$. Note, loss function may be non-convex, thus global optimal is not gauranteed   RNNs: takes both current input $\\mathbf{x}^{(t)}$ and hidden state from previous $\\mathbf{h}^{(t-1)}$:  $h^{(t)} = \\sigma(W^{hx}\\mathbf{x}^{(t)}+W^{hh}\\mathbf{h}^{(t-1)} + \\mathbf{b}_h)$ $\\hat{\\mathbf{y}}^{t}= softmax(W^{yh}\\mathbf{h}^{(t)} + \\mathbf{b}_y)$ With above forward network, backprop of RNN is called Backprop through time (BPTT).   Exploding/vanishing gradients: When learning long-distance dependencies, the gradients of weights may explode or vanish through the time steps. The following tutorial also gives a detailed description. Neural Machine Translation and Sequence-to-sequence Models: A Tutorial.  LSTMs, GRUs, Bidirectional A memory cell and gates are introduced in LSTM, which has unit gradient when adding previous memory $\\mathbf{c}_{t-1}$ to the current $\\mathbf{c}_t$. Bidirectional structure has one more layer of hidden nodes which takes inputs from the end, which allows context information before and after the current step.\nGated recurrent units (GRUs) appear as a simpler and computationally easier alternative of LSTM, which was not introducted in this paper but has on par performance as LSTM.\nNeural Turing Machines Incorporate external memory buffer compared to RNNs.\n Memory matrix + controller(may be a rnn) differentiable end-to-end can backprop via SGD outperforms LSTM (without external memory) when generalizing on longer inputs.  Applications  representing natural language inputs and outputs  character-level, word level, one-hot, word embeddings   BLEU score for translation evaluation  vrevity penalty, ngram precisions METEOR as an alternative which is based on word to word matches   translation as a SEQ2SEQ problem image captioning: input images and target captions as inputs. Encoding can be dong using CNN for images. Attention mechanism can be used in decoding. More: handwriting recognition, unsupervised video encoding, video captioning, computer programs reading.  discussion I don\u0026rsquo;t include most of the equations in the notes. The paper is clear in why and how RNN makes a difference in learning long-dependencies, which serves pretty well as a review/tutorial from a higher level with math well explained.\n","date":1528848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://wenyanli.org/post/getting-started/","publishdate":"2018-06-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Notes on the paper: A Critical Review of Recurrent Neural Networks for Sequence Learning (Lipton and Berkowitz) - about RNN architectures, algorithms, results, comparison to alternative methodologies on sequence learning.","tags":["reading-notes"],"title":"RNN review paper notes","type":"post"},{"authors":null,"categories":null,"content":"Summary In Yu M. et al. (2016), phenotype is translated from genotype based on gene ontology, and predicted interaction scores may be influenced by errors in gene annotations or relationship between terms. As deep learning being effective in identifying complex patterns from feature-rich datasets, especially as recurrent neural networks(RNNs) such as long short term memory(LSTM) and gated recurrent unit(GRU) are capable of dealing with long-distance sequential data, predicting genetic interactions directly from DNA or amino-acid sequences using deep learning techniques would help us gain insights into underlying complex phenotypes.\n","date":1517011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517011200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://wenyanli.org/project/example/","publishdate":"2018-01-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"Predicting pairwise gene interactions using attention-based RNN/CNN, and compared to baseline approach with random forest procedure with gene ontotype described in [Yu, et al. (Cell Systems, 2016)](http://www.cell.com/cell-systems/abstract/S2405-4712(16)30033-3) on the *S. cerevisiae* data from [Costanzo, et al. (Science, 2010)](http://science.sciencemag.org/content/327/5964/425.long).","tags":["Deep Learning"],"title":"Predicting Phenotype from Genomic Sequence with Deep Neural Networks","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://wenyanli.org/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]